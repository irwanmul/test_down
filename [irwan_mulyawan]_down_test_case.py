# -*- coding: utf-8 -*-
"""[Irwan Mulyawan] Down Test Case_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XP0veaEHp_w8Si0cgHa607J3-_WwHNuH
"""

#@title Connecting Collab with GDrive
from google.colab import drive
drive.mount('/content/drive')

cd drive/'My Drive/Test/Down Dating Apps'

"""#Data Loading and Setup"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('train.csv', names=['label', 'content', 'detail'])

"""# Understanding the Data
As you can see we have the same amount of unique data for all four categories
"""

# Plotting the distribution of news categories
# sns.countplot(x='label', data=train)
# plt.title('Distribution of News Categories')
# plt.show()


print("Number of duplicate rows:", train.duplicated().sum())

# Remove duplicates
train_data_clean = train.drop_duplicates()

# Plot the count plot with the cleaned data
sns.countplot(x='label', data=train_data_clean)
plt.title('Unique Distribution of News Categories')
plt.xlabel('Category')
plt.ylabel('Number of Articles')
plt.show()

"""# Text Characteristics"""

# Average word count per category
train['word_count'] = train['content'].apply(lambda x: len(x.split()))
sns.boxplot(x='label', y='word_count', data=train)
plt.title('Word Count by Category')
plt.show()

"""# Trend Analysis
Investigate if there are common words or phrases within categories using a word cloud.
"""

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import nltk

# Ensure that nltk's resources are downloaded
nltk.download('stopwords')

data = pd.read_csv('train.csv', header=None, names=['label', 'content', 'detail'])

# Preprocessing steps
data['processed_content'] = data['content'].str.lower().str.replace(r'\W', ' ').str.replace(r'\s+', ' ')
stop_words = set(stopwords.words('english'))
data['processed_content'] = data['processed_content'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define a function to plot word clouds
def plot_word_cloud(category):
    text = data[data['label'] == category]['processed_content'].str.cat(sep=' ')
    wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for Category {category}')
    plt.show()

# Plot word clouds for each category
for label in data['label'].unique():
    plot_word_cloud(label)

# Function to extract top n-grams
def get_top_ngrams(corpus, n=None, ngrams=2):
    vec = CountVectorizer(ngram_range=(ngrams, ngrams)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

# Example of extracting top bi-grams for each category
for label in data['label'].unique():
    common_phrases = get_top_ngrams(data[data['label'] == label]['processed_content'], n=10, ngrams=2)
    print(f"Top 10 bi-grams for Category {label}:")
    for phrase, freq in common_phrases:
        print(f"{phrase}: {freq}")

"""# Statistical Validation
Using statistical tests (like chi-square tests for categorical data) to validate if the differences observed in distributions (e.g., word count across categories) are statistically significant.
"""

import numpy as np

# Load the dataset
data = pd.read_csv('train.csv', header=None, names=['label', 'content', 'detail'])

# Process text to count words
data['word_count'] = data['content'].apply(lambda x: len(x.split()))

# Categorize word counts to make them definite
# data['word_count_bin'] = pd.cut(data['word_count'], bins=[0, 100, 200, 300, 400, np.inf], labels=['0-100', '101-200', '201-300', '301-400', '400+'])
data['word_count_bin'] = pd.cut(data['word_count'], bins=[0, 5, 10, 15, 20, np.inf], labels=['0-5', '6-10', '11-15', '16-20', '21+'])

from scipy.stats import chi2_contingency

# Create a contingency table
contingency_table = pd.crosstab(data['label'], data['word_count_bin'])

print("Contingency Table:")
print(contingency_table)

# Apply the Chi-square test
chi2, p, dof, expected = chi2_contingency(contingency_table)

print("\nChi-square Statistic:", chi2)
print("Degrees of Freedom:", dof)
print("P-value:", p)

"""# Visualization findings"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Load your dataset
data = pd.read_csv('train.csv', header=None, names=['label', 'content','detail'])

# Terms you want to visualize
terms = ['economy', 'basket', 'technology', 'health', 'war', 'oil', 'us', 'russia']

# Create a document-term matrix
vectorizer = CountVectorizer(vocabulary=terms, binary=False)
dtm = vectorizer.fit_transform(data['content'].str.lower())  # Using str.lower to normalize the text

# Create a DataFrame for easier manipulation
dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())
dtm_df['category'] = data['label']

# Group by category and sum occurrences
grouped = dtm_df.groupby('category').sum()
grouped = grouped.rename(index={1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'})
grouped

import matplotlib.pyplot as plt

grouped.plot(kind='bar', figsize=(12, 6))
plt.title('Frequency of Selected Terms Across News Categories')
plt.xlabel('Category')
plt.ylabel('Number of Occurrences')
plt.xticks(rotation=0)  # Keeps category names horizontal
plt.show()

correlation_matrix = dtm_df[terms].corr()

# Plotting
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Between Terms')
plt.show()

"""# Bonus"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load data
train_data = pd.read_csv('train.csv', header=None, names=['label', 'content', 'detail'])
test_data = pd.read_csv('test.csv', header=None, names=['label', 'content', 'detail'])

# Preview the data
print(train_data.head())

"""### Text Preprocessing
Will convert all text to lowercase, removing punctuation, and vectorizing the text.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Text preprocessing and vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)

X_train = vectorizer.fit_transform(train_data['content'])
y_train = train_data['label']
X_test = vectorizer.transform(test_data['content'])
y_test = test_data['label']

"""### Training the Model
Start with a simple and effective model like Multinomial Naive Bayes, which is commonly used for text classification tasks.
"""

from sklearn.naive_bayes import MultinomialNB

# Initialize the classifier
model = MultinomialNB()

# Train the model
model.fit(X_train, y_train)

"""### Model Evaluation
Evaluate the model on the test data to see how well it generalizes.
"""

from sklearn.metrics import classification_report, accuracy_score

# Predict on the test set
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))